README for Project 3, CS529 - Machine Learning
Anthony Galczak - agalczak@unm.edu - WGalczak@gmail.com
Tristin Glunt - tglunt3unm.edu


Required libraries for execution:
An Nvidia CUDA-enabled GPU (Tristin and I have a GTX1060 and GTX1050 respectively for example)
Tensorflow GPU and it's related libraries (CuDNN, CUDA Toolkit, CUPTI)
- Please reference: https://www.tensorflow.org/install/gpu
Anaconda (at least 4.5.11)
- We need NumPy, SciPy, matplotlib, keras
librosa (at least 0.6.2)


Overview of files available in this repo:
main.py
This file is what is used to start processing data, specifically the .au files we were given.
There are two boolean flags at the top that decide whether you make images out of the .au files and
whether you convert these images into numpy arrays (pickled as .dat files). This is also where we
decide what split we will use for the sound/image files. Split size of 3 gives you 10 3s files for
processing.

ann_classify.ipynb
This is the Jupyter notebook where we train a deep Artificial Neural Network.

aws_classify.ipynb
This is the Jupyter notebook that is setup to use a high-powered AWS server. Currently the p2.xlarge
instance has a K80 GPU to use so it is ~10x more powerful than my GTX1050 or Tristin's GTX1060.
Thus, the network is a bit wider, deeper and higher fidelity than the others.

cnn_classify.ipynb
This is the Jupyter notebook where we train a deep Convolutional Neural Network.

classify_kaggle_data.py
Here we take the output from our Jupyter notebook and actually output our predictions to a .csv file.
The complicated part that is done here is the voting between the split pieces of files. Since we might
have 6, 10, 15 split files we need to vote on a classification between the files.  


Directory's holding data: (These may or may not be available due to disk space requirements)
grey/
Gray-scale split images available for processing.

color/
"New" color images available for processing. cmap = 'nipy_spectral'

mel/
Mel spectrograms that are generated by librosa library. These are color representations of data.


High-level overview of our program flow:
This project has an enormous amount of configurability and hyperparameters to tune. As such there is
no command line interface for this program as most of the hyperparameters are tunable via Jupyter
notebooks. CNNs have a ton of hyperparameters that can be optimized and we have spent a lot of time
doing this.

Run main.py against data with make_images = True and converting = True.
This will take some time, ~15 minutes to generate split .png files and .dat pickled numpy array files
for processing by the network you want to use.

Run the Jupyter notebook corresponding to the type of network you want to use. The most realistic one
to use is cnn_classify.ipynb.
The very first cell will verify that you are using a CUDA-enabled GPU. If you are not, none of this will work.
Inside the Jupyter notebook you will want to analyze the cell under "Build the CNN network".
This cell will allow you to tune how many convolutions to use, what size of convolutions (3x3, 7x7, 11x11, etc.),
how many layers to use as well as various techniques for reducing overfitting (dropout, batch_normalization, etc.).
Then, there is another cell to look at for processing, "Begin training the mode".
Here, you can select things like where do you want your model to be saved, how many epochs to run and what
optimizer to use (sgd, adam, etc.).
After running this cell you will now be training. Depending on your selections you will likely be waiting anywhere
from 15mins to many hours.
This part of the process utilizes a GPU for computation. This speeds up computation _significantly_.




