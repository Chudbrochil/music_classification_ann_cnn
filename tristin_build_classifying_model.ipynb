{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 'jupyter notebook' from Conda Terminal before beginning to use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17466363671954521762\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4952306483\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6641045901166123404\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "Tensorflow:  1.11.0\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "# Testing to make sure TensorFlow GPU is working\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "print('Tensorflow: ', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data, shuffle data, normalize data, split data into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2175.02383311 2103.86995762 1955.28566681 ... 4031.3645376\n",
      "  3856.51368531 3270.84446624]\n",
      " [2570.77299051 2533.11347523 2451.78178665 ... 1849.5708254\n",
      "  1685.14410446 1589.4784735 ]\n",
      " [1635.23782519 1523.11687881 1369.41189919 ... 1503.12816303\n",
      "  1483.58459923 1365.20503472]\n",
      " ...\n",
      " [3535.57191916 2462.27935639 1529.4621064  ...  874.58988997\n",
      "   839.80340766  814.89165223]\n",
      " [ 749.24281579 1764.1412354  2563.01131115 ... 1431.25337975\n",
      "  1311.69600294 1063.08285373]\n",
      " [ 980.470122   1030.84782814 1130.95585045 ...  969.65235904\n",
      "   928.0298142   893.33106738]]\n",
      "(8996, 130)\n",
      "(8996, 10)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f85f9edad618>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# split data into validation set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mtraining_set_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m.80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mX_train_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraining_set_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0my_train_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraining_set_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtraining_set_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Load the data\n",
    "X_train = np.load(\"X_melnew_train_3.dat\")\n",
    "y_train = np.load(\"y_melnew_train_3.dat\")\n",
    "\n",
    "print(X_train)\n",
    "\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=16)\n",
    "# reshape so in form for CNN-Keras\n",
    "#X_train = X_train.reshape(X_train.shape[0], 174, 124, 1)\n",
    "\n",
    "# Normalize the data\n",
    "# X_train = X_train/255\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n",
    "# split data into validation set\n",
    "training_set_size = int(X_train.shape[0] * .80)\n",
    "X_train_validation = X_train[training_set_size:, :,:,:]\n",
    "y_train_validation = y_train[training_set_size:, :]\n",
    "y_train = y_train[:training_set_size,:]\n",
    "X_train = X_train[:training_set_size, :, :, :]\n",
    "print(\"Validation x train set:\" + str(X_train_validation.shape))\n",
    "print(\"X train set:\" + str(X_train.shape))\n",
    "print(\"Validation y train set:\" + str(y_train_validation.shape))\n",
    "print(\"Y train set:\" + str(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the CNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG-16 like network from Andrew Ng's course on Coursera\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "    # this applies 32 convolution filters of size 3x3 each.\n",
    "    model.add(BatchNormalization(input_shape=X_train.shape[1:]))\n",
    "    \n",
    "    model.add(Conv2D(64, (7,7), activation = 'relu', strides=(1,1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(128, (7,7), activation = 'relu', strides=(1,1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())    \n",
    "    # model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(256, (7,7), activation = 'relu', strides=(1,1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.5))\n",
    "\n",
    "#     model.add(Conv2D(512, (2,2), activation = 'relu', strides=(1,1)))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.5))\n",
    "\n",
    "    # model.add(Conv2D(256, (7, 7), activation = 'relu', strides=(1,1)))\n",
    "    # # # model.add(BatchNormalization())\n",
    "    # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # # # model.add(Dropout(0.5))\n",
    "\n",
    "    # # FLATTEN X (means convert it to a vector) + FULLYCONNECTED\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "225/225 [==============================] - 51s 229ms/step - loss: 2.3905 - acc: 0.3136 - val_loss: 1.8831 - val_acc: 0.4156\n",
      "Epoch 2/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 1.9087 - acc: 0.3936 - val_loss: 2.1276 - val_acc: 0.3189\n",
      "Epoch 3/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 1.6886 - acc: 0.4383 - val_loss: 1.5556 - val_acc: 0.4711\n",
      "Epoch 4/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 1.5658 - acc: 0.4700 - val_loss: 1.5575 - val_acc: 0.4561\n",
      "Epoch 5/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 1.4951 - acc: 0.4889 - val_loss: 1.6308 - val_acc: 0.4244\n",
      "Epoch 6/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 1.4244 - acc: 0.5136 - val_loss: 1.5942 - val_acc: 0.4400\n",
      "Epoch 7/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 1.3917 - acc: 0.5324 - val_loss: 1.4692 - val_acc: 0.4900\n",
      "Epoch 8/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 1.3532 - acc: 0.5321 - val_loss: 1.3649 - val_acc: 0.5250\n",
      "Epoch 9/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 1.3047 - acc: 0.5433 - val_loss: 1.4381 - val_acc: 0.5322\n",
      "Epoch 10/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 1.2740 - acc: 0.5565 - val_loss: 1.4030 - val_acc: 0.5206\n",
      "Epoch 11/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 1.2509 - acc: 0.5689 - val_loss: 1.2429 - val_acc: 0.5817\n",
      "Epoch 12/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 1.2359 - acc: 0.5690 - val_loss: 1.2814 - val_acc: 0.5611\n",
      "Epoch 13/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 1.2192 - acc: 0.5806 - val_loss: 1.5320 - val_acc: 0.5122\n",
      "Epoch 14/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 1.2062 - acc: 0.5804 - val_loss: 1.4841 - val_acc: 0.5144\n",
      "Epoch 15/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 1.1770 - acc: 0.5943 - val_loss: 1.1896 - val_acc: 0.5911\n",
      "Epoch 16/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 1.1788 - acc: 0.6031 - val_loss: 1.2273 - val_acc: 0.5883\n",
      "Epoch 17/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 1.1403 - acc: 0.6078 - val_loss: 1.3914 - val_acc: 0.5422\n",
      "Epoch 18/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 1.1208 - acc: 0.6194 - val_loss: 1.3644 - val_acc: 0.5533\n",
      "Epoch 19/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 1.1250 - acc: 0.6182 - val_loss: 1.1907 - val_acc: 0.6111\n",
      "Epoch 20/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 1.0975 - acc: 0.6244 - val_loss: 1.2903 - val_acc: 0.5717\n",
      "Epoch 21/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 1.0743 - acc: 0.6219 - val_loss: 1.3539 - val_acc: 0.5722\n",
      "Epoch 22/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 1.0847 - acc: 0.6339 - val_loss: 1.1953 - val_acc: 0.5928\n",
      "Epoch 23/1000\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 1.0795 - acc: 0.6344 - val_loss: 1.3748 - val_acc: 0.5722\n",
      "Epoch 24/1000\n",
      "225/225 [==============================] - 48s 213ms/step - loss: 1.0488 - acc: 0.6350 - val_loss: 1.3415 - val_acc: 0.5778\n",
      "Epoch 25/1000\n",
      "225/225 [==============================] - 48s 213ms/step - loss: 1.0550 - acc: 0.6404 - val_loss: 1.3040 - val_acc: 0.5700\n",
      "Epoch 26/1000\n",
      "225/225 [==============================] - 48s 213ms/step - loss: 1.0370 - acc: 0.6467 - val_loss: 1.0915 - val_acc: 0.6322\n",
      "Epoch 27/1000\n",
      "225/225 [==============================] - 48s 213ms/step - loss: 1.0125 - acc: 0.6471 - val_loss: 1.4307 - val_acc: 0.5361\n",
      "Epoch 28/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 1.0210 - acc: 0.6507 - val_loss: 1.0854 - val_acc: 0.6300\n",
      "Epoch 29/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 1.0080 - acc: 0.6539 - val_loss: 1.1705 - val_acc: 0.6067\n",
      "Epoch 30/1000\n",
      "225/225 [==============================] - 48s 213ms/step - loss: 1.0091 - acc: 0.6512 - val_loss: 1.3272 - val_acc: 0.5789\n",
      "Epoch 31/1000\n",
      "225/225 [==============================] - 48s 213ms/step - loss: 0.9824 - acc: 0.6617 - val_loss: 1.3578 - val_acc: 0.5644\n",
      "Epoch 32/1000\n",
      "225/225 [==============================] - 48s 213ms/step - loss: 0.9963 - acc: 0.6593 - val_loss: 1.0312 - val_acc: 0.6606\n",
      "Epoch 33/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.9786 - acc: 0.6614 - val_loss: 1.2751 - val_acc: 0.5783\n",
      "Epoch 34/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.9616 - acc: 0.6706 - val_loss: 1.1813 - val_acc: 0.6189\n",
      "Epoch 35/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.9610 - acc: 0.6669 - val_loss: 1.0944 - val_acc: 0.6244\n",
      "Epoch 36/1000\n",
      "225/225 [==============================] - 48s 213ms/step - loss: 0.9499 - acc: 0.6751 - val_loss: 1.1882 - val_acc: 0.6244\n",
      "Epoch 37/1000\n",
      "225/225 [==============================] - 49s 218ms/step - loss: 0.9278 - acc: 0.6785 - val_loss: 1.4017 - val_acc: 0.5389\n",
      "Epoch 38/1000\n",
      "225/225 [==============================] - 51s 228ms/step - loss: 0.9332 - acc: 0.6818 - val_loss: 1.5779 - val_acc: 0.5478\n",
      "Epoch 39/1000\n",
      "225/225 [==============================] - 49s 219ms/step - loss: 0.9405 - acc: 0.6714 - val_loss: 1.2984 - val_acc: 0.5700\n",
      "Epoch 40/1000\n",
      "225/225 [==============================] - 49s 218ms/step - loss: 0.8817 - acc: 0.6929 - val_loss: 1.1892 - val_acc: 0.6089\n",
      "Epoch 41/1000\n",
      "225/225 [==============================] - 51s 226ms/step - loss: 0.9111 - acc: 0.6826 - val_loss: 1.2233 - val_acc: 0.6122\n",
      "Epoch 42/1000\n",
      "225/225 [==============================] - 52s 231ms/step - loss: 0.9129 - acc: 0.6853 - val_loss: 1.1950 - val_acc: 0.6083\n",
      "Epoch 43/1000\n",
      "225/225 [==============================] - 55s 244ms/step - loss: 0.9247 - acc: 0.6818 - val_loss: 1.0552 - val_acc: 0.6517\n",
      "Epoch 44/1000\n",
      "225/225 [==============================] - 55s 244ms/step - loss: 0.8731 - acc: 0.6974 - val_loss: 1.1936 - val_acc: 0.6228\n",
      "Epoch 45/1000\n",
      "225/225 [==============================] - 50s 220ms/step - loss: 0.8775 - acc: 0.6987 - val_loss: 1.2925 - val_acc: 0.6139\n",
      "Epoch 46/1000\n",
      "225/225 [==============================] - 49s 218ms/step - loss: 0.8818 - acc: 0.6965 - val_loss: 1.1165 - val_acc: 0.6500\n",
      "Epoch 47/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.8633 - acc: 0.7050 - val_loss: 1.0214 - val_acc: 0.6733\n",
      "Epoch 48/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.8681 - acc: 0.6987 - val_loss: 1.2102 - val_acc: 0.6222\n",
      "Epoch 49/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.8442 - acc: 0.7031 - val_loss: 1.0742 - val_acc: 0.6544\n",
      "Epoch 50/1000\n",
      "225/225 [==============================] - 50s 221ms/step - loss: 0.8316 - acc: 0.7104 - val_loss: 1.1336 - val_acc: 0.6350\n",
      "Epoch 51/1000\n",
      "225/225 [==============================] - 49s 219ms/step - loss: 0.8505 - acc: 0.7074 - val_loss: 1.2078 - val_acc: 0.6217\n",
      "Epoch 52/1000\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.8467 - acc: 0.7133 - val_loss: 1.2632 - val_acc: 0.6100\n",
      "Epoch 53/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.8402 - acc: 0.7042 - val_loss: 1.1736 - val_acc: 0.6422\n",
      "Epoch 54/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.8291 - acc: 0.7128 - val_loss: 1.1559 - val_acc: 0.6217\n",
      "Epoch 55/1000\n",
      "225/225 [==============================] - 48s 216ms/step - loss: 0.8169 - acc: 0.7207 - val_loss: 1.3727 - val_acc: 0.6106\n",
      "Epoch 56/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.8203 - acc: 0.7140 - val_loss: 1.1968 - val_acc: 0.6428\n",
      "Epoch 57/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.8049 - acc: 0.7229 - val_loss: 1.0583 - val_acc: 0.6561\n",
      "Epoch 58/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.7900 - acc: 0.7285 - val_loss: 1.0652 - val_acc: 0.6611\n",
      "Epoch 59/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.8111 - acc: 0.7168 - val_loss: 1.5977 - val_acc: 0.5539\n",
      "Epoch 60/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.7989 - acc: 0.7231 - val_loss: 1.0274 - val_acc: 0.6783\n",
      "Epoch 61/1000\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.7796 - acc: 0.7274 - val_loss: 1.0188 - val_acc: 0.6756\n",
      "Epoch 62/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.7814 - acc: 0.7319 - val_loss: 1.3080 - val_acc: 0.6106\n",
      "Epoch 63/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.8015 - acc: 0.7274 - val_loss: 0.9915 - val_acc: 0.6789\n",
      "Epoch 64/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.7773 - acc: 0.7318 - val_loss: 1.1235 - val_acc: 0.6578\n",
      "Epoch 65/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.7646 - acc: 0.7331 - val_loss: 1.0514 - val_acc: 0.6794\n",
      "Epoch 66/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.7652 - acc: 0.7333 - val_loss: 1.1075 - val_acc: 0.6561\n",
      "Epoch 67/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.7620 - acc: 0.7346 - val_loss: 1.1447 - val_acc: 0.6383\n",
      "Epoch 68/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.7673 - acc: 0.7346 - val_loss: 1.0343 - val_acc: 0.6856\n",
      "Epoch 69/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.7532 - acc: 0.7443 - val_loss: 0.9749 - val_acc: 0.6861\n",
      "Epoch 70/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.7446 - acc: 0.7431 - val_loss: 1.1532 - val_acc: 0.6533\n",
      "Epoch 71/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.7271 - acc: 0.7465 - val_loss: 1.0118 - val_acc: 0.6861\n",
      "Epoch 72/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.7481 - acc: 0.7357 - val_loss: 1.0374 - val_acc: 0.6794\n",
      "Epoch 73/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.7277 - acc: 0.7462 - val_loss: 0.9928 - val_acc: 0.7006\n",
      "Epoch 74/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.7267 - acc: 0.7496 - val_loss: 1.2059 - val_acc: 0.6617\n",
      "Epoch 75/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.7216 - acc: 0.7512 - val_loss: 0.9357 - val_acc: 0.7056\n",
      "Epoch 76/1000\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.7046 - acc: 0.7544 - val_loss: 0.8954 - val_acc: 0.7083\n",
      "Epoch 77/1000\n",
      "225/225 [==============================] - 48s 216ms/step - loss: 0.7182 - acc: 0.7560 - val_loss: 1.1330 - val_acc: 0.6683\n",
      "Epoch 78/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.7238 - acc: 0.7528 - val_loss: 1.1018 - val_acc: 0.6789\n",
      "Epoch 79/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.7152 - acc: 0.7582 - val_loss: 1.0328 - val_acc: 0.6839\n",
      "Epoch 80/1000\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.6965 - acc: 0.7593 - val_loss: 1.0052 - val_acc: 0.7072\n",
      "Epoch 81/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.6979 - acc: 0.7614 - val_loss: 0.9955 - val_acc: 0.6967\n",
      "Epoch 82/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.6823 - acc: 0.7618 - val_loss: 1.1326 - val_acc: 0.6594\n",
      "Epoch 83/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.6798 - acc: 0.7644 - val_loss: 0.9788 - val_acc: 0.7061\n",
      "Epoch 84/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.6962 - acc: 0.7610 - val_loss: 0.8306 - val_acc: 0.7328\n",
      "Epoch 85/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.6753 - acc: 0.7656 - val_loss: 1.3667 - val_acc: 0.5772\n",
      "Epoch 86/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.6805 - acc: 0.7611 - val_loss: 0.9969 - val_acc: 0.6928\n",
      "Epoch 87/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.6759 - acc: 0.7669 - val_loss: 1.0153 - val_acc: 0.6906\n",
      "Epoch 88/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.6632 - acc: 0.7714 - val_loss: 1.2084 - val_acc: 0.6467\n",
      "Epoch 89/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.6824 - acc: 0.7646 - val_loss: 0.9271 - val_acc: 0.7089\n",
      "Epoch 90/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.6687 - acc: 0.7696 - val_loss: 0.9860 - val_acc: 0.7078\n",
      "Epoch 91/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.6478 - acc: 0.7801 - val_loss: 1.2026 - val_acc: 0.6572\n",
      "Epoch 92/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.6413 - acc: 0.7736 - val_loss: 1.2962 - val_acc: 0.6500\n",
      "Epoch 93/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.6559 - acc: 0.7753 - val_loss: 1.1622 - val_acc: 0.6811\n",
      "Epoch 94/1000\n",
      "225/225 [==============================] - 48s 216ms/step - loss: 0.6375 - acc: 0.7756 - val_loss: 1.0647 - val_acc: 0.6939\n",
      "Epoch 95/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.6572 - acc: 0.7707 - val_loss: 1.4617 - val_acc: 0.6211\n",
      "Epoch 96/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.6446 - acc: 0.7764 - val_loss: 1.1985 - val_acc: 0.6728\n",
      "Epoch 97/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.6488 - acc: 0.7769 - val_loss: 1.0760 - val_acc: 0.6956\n",
      "Epoch 98/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.6240 - acc: 0.7828 - val_loss: 1.0323 - val_acc: 0.7044\n",
      "Epoch 99/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.6300 - acc: 0.7815 - val_loss: 0.8077 - val_acc: 0.7550\n",
      "Epoch 100/1000\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.6162 - acc: 0.7890 - val_loss: 1.0815 - val_acc: 0.6861\n",
      "Epoch 101/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.6163 - acc: 0.7890 - val_loss: 1.1016 - val_acc: 0.6872\n",
      "Epoch 102/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.6168 - acc: 0.7922 - val_loss: 0.9339 - val_acc: 0.7217\n",
      "Epoch 103/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.6333 - acc: 0.7812 - val_loss: 1.0139 - val_acc: 0.7100\n",
      "Epoch 104/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.6179 - acc: 0.7760 - val_loss: 0.8328 - val_acc: 0.7506\n",
      "Epoch 105/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.6071 - acc: 0.7881 - val_loss: 1.0770 - val_acc: 0.6917\n",
      "Epoch 106/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.6179 - acc: 0.7835 - val_loss: 0.7765 - val_acc: 0.7550\n",
      "Epoch 107/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.5883 - acc: 0.7894 - val_loss: 1.0086 - val_acc: 0.6972\n",
      "Epoch 108/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.6008 - acc: 0.7919 - val_loss: 1.2109 - val_acc: 0.6578\n",
      "Epoch 109/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.5970 - acc: 0.7915 - val_loss: 1.0642 - val_acc: 0.6867\n",
      "Epoch 110/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.5864 - acc: 0.7989 - val_loss: 0.9235 - val_acc: 0.7389\n",
      "Epoch 111/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.6077 - acc: 0.7910 - val_loss: 1.0058 - val_acc: 0.7039\n",
      "Epoch 112/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5916 - acc: 0.7896 - val_loss: 1.0460 - val_acc: 0.7078\n",
      "Epoch 113/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5878 - acc: 0.7899 - val_loss: 1.3032 - val_acc: 0.6706\n",
      "Epoch 114/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5837 - acc: 0.8028 - val_loss: 1.2869 - val_acc: 0.6611\n",
      "Epoch 115/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.5814 - acc: 0.7961 - val_loss: 0.9907 - val_acc: 0.7189\n",
      "Epoch 116/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.5641 - acc: 0.8032 - val_loss: 1.1723 - val_acc: 0.6706\n",
      "Epoch 117/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5717 - acc: 0.8024 - val_loss: 0.8867 - val_acc: 0.7389\n",
      "Epoch 118/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.5668 - acc: 0.7996 - val_loss: 0.9823 - val_acc: 0.7189\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5685 - acc: 0.8065 - val_loss: 1.6139 - val_acc: 0.5778\n",
      "Epoch 120/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5754 - acc: 0.7982 - val_loss: 1.3688 - val_acc: 0.6533\n",
      "Epoch 121/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5682 - acc: 0.8031 - val_loss: 1.0769 - val_acc: 0.6967\n",
      "Epoch 122/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5623 - acc: 0.8015 - val_loss: 1.1406 - val_acc: 0.6950\n",
      "Epoch 123/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5746 - acc: 0.8003 - val_loss: 0.9058 - val_acc: 0.7300\n",
      "Epoch 124/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5379 - acc: 0.8136 - val_loss: 1.1451 - val_acc: 0.6872\n",
      "Epoch 125/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5527 - acc: 0.8081 - val_loss: 1.0446 - val_acc: 0.7039\n",
      "Epoch 126/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5316 - acc: 0.8168 - val_loss: 1.2665 - val_acc: 0.6556\n",
      "Epoch 127/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5450 - acc: 0.8118 - val_loss: 0.9776 - val_acc: 0.7094\n",
      "Epoch 128/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5460 - acc: 0.8113 - val_loss: 1.0285 - val_acc: 0.6933\n",
      "Epoch 129/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5602 - acc: 0.8047 - val_loss: 1.0218 - val_acc: 0.7189\n",
      "Epoch 130/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5320 - acc: 0.8125 - val_loss: 1.1746 - val_acc: 0.6883\n",
      "Epoch 131/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5552 - acc: 0.8125 - val_loss: 1.0544 - val_acc: 0.7228\n",
      "Epoch 132/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5299 - acc: 0.8146 - val_loss: 1.1498 - val_acc: 0.7022\n",
      "Epoch 133/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5422 - acc: 0.8172 - val_loss: 0.8028 - val_acc: 0.7650\n",
      "Epoch 134/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.5186 - acc: 0.8197 - val_loss: 0.9341 - val_acc: 0.7378\n",
      "Epoch 135/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5173 - acc: 0.8182 - val_loss: 1.1167 - val_acc: 0.6939\n",
      "Epoch 136/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.5176 - acc: 0.8206 - val_loss: 1.1811 - val_acc: 0.6717\n",
      "Epoch 137/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5408 - acc: 0.8079 - val_loss: 1.0112 - val_acc: 0.7256\n",
      "Epoch 138/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5406 - acc: 0.8125 - val_loss: 1.1043 - val_acc: 0.6911\n",
      "Epoch 139/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5172 - acc: 0.8139 - val_loss: 1.0268 - val_acc: 0.6906\n",
      "Epoch 140/1000\n",
      "225/225 [==============================] - 48s 213ms/step - loss: 0.5136 - acc: 0.8231 - val_loss: 1.0064 - val_acc: 0.7122\n",
      "Epoch 141/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5136 - acc: 0.8214 - val_loss: 1.5911 - val_acc: 0.6361\n",
      "Epoch 142/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5161 - acc: 0.8242 - val_loss: 0.7860 - val_acc: 0.7656\n",
      "Epoch 143/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.5006 - acc: 0.8243 - val_loss: 1.1865 - val_acc: 0.6700\n",
      "Epoch 144/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4984 - acc: 0.8206 - val_loss: 0.9976 - val_acc: 0.7333\n",
      "Epoch 145/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4975 - acc: 0.8300 - val_loss: 1.3156 - val_acc: 0.6767\n",
      "Epoch 146/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.4970 - acc: 0.8249 - val_loss: 0.9415 - val_acc: 0.7367\n",
      "Epoch 147/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.4966 - acc: 0.8268 - val_loss: 1.2594 - val_acc: 0.6722\n",
      "Epoch 148/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.5129 - acc: 0.8237 - val_loss: 0.9874 - val_acc: 0.7306\n",
      "Epoch 149/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4827 - acc: 0.8265 - val_loss: 1.1974 - val_acc: 0.6761\n",
      "Epoch 150/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4841 - acc: 0.8358 - val_loss: 1.0237 - val_acc: 0.7117\n",
      "Epoch 151/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4966 - acc: 0.8262 - val_loss: 1.0444 - val_acc: 0.7167\n",
      "Epoch 152/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4839 - acc: 0.8307 - val_loss: 1.2581 - val_acc: 0.6789\n",
      "Epoch 153/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4812 - acc: 0.8336 - val_loss: 1.0673 - val_acc: 0.7122\n",
      "Epoch 154/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.4588 - acc: 0.8379 - val_loss: 0.8591 - val_acc: 0.7422\n",
      "Epoch 155/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4666 - acc: 0.8417 - val_loss: 1.1374 - val_acc: 0.7050\n",
      "Epoch 156/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4735 - acc: 0.8329 - val_loss: 1.1183 - val_acc: 0.7111\n",
      "Epoch 157/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4715 - acc: 0.8386 - val_loss: 0.8983 - val_acc: 0.7506\n",
      "Epoch 158/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4743 - acc: 0.8315 - val_loss: 0.9118 - val_acc: 0.7417\n",
      "Epoch 159/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4735 - acc: 0.8379 - val_loss: 1.0950 - val_acc: 0.7156\n",
      "Epoch 160/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4720 - acc: 0.8356 - val_loss: 0.9303 - val_acc: 0.7539\n",
      "Epoch 161/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4722 - acc: 0.8387 - val_loss: 1.1311 - val_acc: 0.7061\n",
      "Epoch 162/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4744 - acc: 0.8371 - val_loss: 0.8782 - val_acc: 0.7528\n",
      "Epoch 163/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4759 - acc: 0.8354 - val_loss: 0.9662 - val_acc: 0.7450\n",
      "Epoch 164/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4578 - acc: 0.8381 - val_loss: 0.9274 - val_acc: 0.7556\n",
      "Epoch 165/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4502 - acc: 0.8414 - val_loss: 0.9657 - val_acc: 0.7494\n",
      "Epoch 166/1000\n",
      "225/225 [==============================] - 48s 213ms/step - loss: 0.4550 - acc: 0.8354 - val_loss: 1.1992 - val_acc: 0.6933\n",
      "Epoch 167/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4524 - acc: 0.8387 - val_loss: 0.9469 - val_acc: 0.7489\n",
      "Epoch 168/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.4435 - acc: 0.8506 - val_loss: 0.9871 - val_acc: 0.7328\n",
      "Epoch 169/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4418 - acc: 0.8413 - val_loss: 0.8642 - val_acc: 0.7539\n",
      "Epoch 170/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4373 - acc: 0.8494 - val_loss: 1.0086 - val_acc: 0.7272\n",
      "Epoch 171/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4390 - acc: 0.8485 - val_loss: 1.9566 - val_acc: 0.5906\n",
      "Epoch 172/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4237 - acc: 0.8518 - val_loss: 0.9673 - val_acc: 0.7322\n",
      "Epoch 173/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4199 - acc: 0.8533 - val_loss: 1.0676 - val_acc: 0.7328\n",
      "Epoch 174/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.4267 - acc: 0.8465 - val_loss: 0.9655 - val_acc: 0.7383\n",
      "Epoch 175/1000\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.4324 - acc: 0.8512 - val_loss: 1.0101 - val_acc: 0.7311\n",
      "Epoch 176/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.4316 - acc: 0.8528 - val_loss: 0.9889 - val_acc: 0.7317\n",
      "Epoch 177/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4272 - acc: 0.8503 - val_loss: 0.7795 - val_acc: 0.7711\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4389 - acc: 0.8500 - val_loss: 0.8516 - val_acc: 0.7567\n",
      "Epoch 179/1000\n",
      "225/225 [==============================] - 48s 213ms/step - loss: 0.4409 - acc: 0.8483 - val_loss: 0.8659 - val_acc: 0.7583\n",
      "Epoch 180/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4186 - acc: 0.8581 - val_loss: 1.1965 - val_acc: 0.6989\n",
      "Epoch 181/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4303 - acc: 0.8528 - val_loss: 1.1884 - val_acc: 0.7111\n",
      "Epoch 182/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.4307 - acc: 0.8465 - val_loss: 1.0294 - val_acc: 0.7233\n",
      "Epoch 183/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.4336 - acc: 0.8514 - val_loss: 1.0425 - val_acc: 0.7294\n",
      "Epoch 184/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.4190 - acc: 0.8542 - val_loss: 1.0979 - val_acc: 0.7183\n",
      "Epoch 185/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4052 - acc: 0.8583 - val_loss: 0.9031 - val_acc: 0.7472\n",
      "Epoch 186/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3975 - acc: 0.8593 - val_loss: 1.0395 - val_acc: 0.7311\n",
      "Epoch 187/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4118 - acc: 0.8554 - val_loss: 0.9168 - val_acc: 0.7594\n",
      "Epoch 188/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.3995 - acc: 0.8613 - val_loss: 0.9723 - val_acc: 0.7406\n",
      "Epoch 189/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4084 - acc: 0.8557 - val_loss: 1.1002 - val_acc: 0.7333\n",
      "Epoch 190/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3991 - acc: 0.8597 - val_loss: 1.0756 - val_acc: 0.7083\n",
      "Epoch 191/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3969 - acc: 0.8647 - val_loss: 1.1177 - val_acc: 0.7078\n",
      "Epoch 192/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.4062 - acc: 0.8574 - val_loss: 0.8639 - val_acc: 0.7611\n",
      "Epoch 193/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4046 - acc: 0.8536 - val_loss: 1.2102 - val_acc: 0.7044\n",
      "Epoch 194/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3925 - acc: 0.8678 - val_loss: 1.1206 - val_acc: 0.7211\n",
      "Epoch 195/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3961 - acc: 0.8621 - val_loss: 1.0568 - val_acc: 0.7233\n",
      "Epoch 196/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3896 - acc: 0.8617 - val_loss: 0.8493 - val_acc: 0.7661\n",
      "Epoch 197/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3928 - acc: 0.8613 - val_loss: 0.9493 - val_acc: 0.7450\n",
      "Epoch 198/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3822 - acc: 0.8675 - val_loss: 1.2944 - val_acc: 0.6928\n",
      "Epoch 199/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4059 - acc: 0.8603 - val_loss: 1.0402 - val_acc: 0.7378\n",
      "Epoch 200/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4020 - acc: 0.8625 - val_loss: 1.0027 - val_acc: 0.7156\n",
      "Epoch 201/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3924 - acc: 0.8654 - val_loss: 1.0512 - val_acc: 0.7361\n",
      "Epoch 202/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3872 - acc: 0.8647 - val_loss: 0.8891 - val_acc: 0.7589\n",
      "Epoch 203/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3789 - acc: 0.8672 - val_loss: 0.8206 - val_acc: 0.7733\n",
      "Epoch 204/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.3914 - acc: 0.8643 - val_loss: 0.9678 - val_acc: 0.7461\n",
      "Epoch 205/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3659 - acc: 0.8714 - val_loss: 1.1821 - val_acc: 0.7039\n",
      "Epoch 206/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3752 - acc: 0.8685 - val_loss: 1.7654 - val_acc: 0.6156\n",
      "Epoch 207/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4096 - acc: 0.8565 - val_loss: 1.0038 - val_acc: 0.7417\n",
      "Epoch 208/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3593 - acc: 0.8753 - val_loss: 0.9410 - val_acc: 0.7561\n",
      "Epoch 209/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3740 - acc: 0.8696 - val_loss: 1.2232 - val_acc: 0.7167\n",
      "Epoch 210/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3606 - acc: 0.8735 - val_loss: 1.0991 - val_acc: 0.7217\n",
      "Epoch 211/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3714 - acc: 0.8701 - val_loss: 0.9985 - val_acc: 0.7461\n",
      "Epoch 212/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3712 - acc: 0.8721 - val_loss: 0.9468 - val_acc: 0.7611\n",
      "Epoch 213/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3550 - acc: 0.8767 - val_loss: 1.0168 - val_acc: 0.7556\n",
      "Epoch 214/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3839 - acc: 0.8619 - val_loss: 1.0516 - val_acc: 0.7328\n",
      "Epoch 215/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3723 - acc: 0.8697 - val_loss: 0.8249 - val_acc: 0.7839\n",
      "Epoch 216/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3581 - acc: 0.8732 - val_loss: 1.4937 - val_acc: 0.6394\n",
      "Epoch 217/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3690 - acc: 0.8719 - val_loss: 1.0179 - val_acc: 0.7667\n",
      "Epoch 218/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3810 - acc: 0.8661 - val_loss: 1.9087 - val_acc: 0.6061\n",
      "Epoch 219/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3601 - acc: 0.8704 - val_loss: 1.0827 - val_acc: 0.7378\n",
      "Epoch 220/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3654 - acc: 0.8744 - val_loss: 0.9213 - val_acc: 0.7656\n",
      "Epoch 221/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.3440 - acc: 0.8778 - val_loss: 0.7369 - val_acc: 0.8000\n",
      "Epoch 222/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.3419 - acc: 0.8772 - val_loss: 0.8226 - val_acc: 0.7822\n",
      "Epoch 223/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3634 - acc: 0.8750 - val_loss: 1.0517 - val_acc: 0.7600\n",
      "Epoch 224/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3370 - acc: 0.8807 - val_loss: 1.0613 - val_acc: 0.7422\n",
      "Epoch 225/1000\n",
      "225/225 [==============================] - 48s 213ms/step - loss: 0.3433 - acc: 0.8828 - val_loss: 1.1655 - val_acc: 0.7278\n",
      "Epoch 226/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3444 - acc: 0.8814 - val_loss: 1.0051 - val_acc: 0.7633\n",
      "Epoch 227/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3513 - acc: 0.8772 - val_loss: 0.8951 - val_acc: 0.7756\n",
      "Epoch 228/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3489 - acc: 0.8768 - val_loss: 1.0230 - val_acc: 0.7450\n",
      "Epoch 229/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.3386 - acc: 0.8862 - val_loss: 0.9500 - val_acc: 0.7778\n",
      "Epoch 230/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3531 - acc: 0.8774 - val_loss: 1.0038 - val_acc: 0.7450\n",
      "Epoch 231/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3438 - acc: 0.8828 - val_loss: 0.9658 - val_acc: 0.7550\n",
      "Epoch 232/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3432 - acc: 0.8836 - val_loss: 0.9588 - val_acc: 0.7500\n",
      "Epoch 233/1000\n",
      "225/225 [==============================] - 48s 213ms/step - loss: 0.3368 - acc: 0.8822 - val_loss: 1.0454 - val_acc: 0.7378\n",
      "Epoch 234/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3429 - acc: 0.8779 - val_loss: 0.8717 - val_acc: 0.7794\n",
      "Epoch 235/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3257 - acc: 0.8831 - val_loss: 1.4729 - val_acc: 0.6828\n",
      "Epoch 236/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3350 - acc: 0.8850 - val_loss: 0.7865 - val_acc: 0.7939\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3203 - acc: 0.8883 - val_loss: 0.8016 - val_acc: 0.7883\n",
      "Epoch 238/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3215 - acc: 0.8904 - val_loss: 0.8553 - val_acc: 0.7828\n",
      "Epoch 239/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3236 - acc: 0.8903 - val_loss: 1.0762 - val_acc: 0.7528\n",
      "Epoch 240/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.3268 - acc: 0.8886 - val_loss: 0.8395 - val_acc: 0.7733\n",
      "Epoch 241/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3352 - acc: 0.8853 - val_loss: 0.9474 - val_acc: 0.7628\n",
      "Epoch 242/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3207 - acc: 0.8897 - val_loss: 1.0996 - val_acc: 0.7417\n",
      "Epoch 243/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3300 - acc: 0.8861 - val_loss: 0.9944 - val_acc: 0.7672\n",
      "Epoch 244/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.3243 - acc: 0.8858 - val_loss: 1.5489 - val_acc: 0.6811\n",
      "Epoch 245/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.3165 - acc: 0.8899 - val_loss: 1.1083 - val_acc: 0.7389\n",
      "Epoch 246/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2958 - acc: 0.8951 - val_loss: 0.9521 - val_acc: 0.7694\n",
      "Epoch 247/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.3073 - acc: 0.8894 - val_loss: 1.1100 - val_acc: 0.7472\n",
      "Epoch 248/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.3070 - acc: 0.8936 - val_loss: 0.9882 - val_acc: 0.7522\n",
      "Epoch 249/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3064 - acc: 0.8904 - val_loss: 1.0057 - val_acc: 0.7550\n",
      "Epoch 250/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2926 - acc: 0.8951 - val_loss: 0.9769 - val_acc: 0.7556\n",
      "Epoch 251/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3023 - acc: 0.8950 - val_loss: 0.8989 - val_acc: 0.7717\n",
      "Epoch 252/1000\n",
      "225/225 [==============================] - 48s 213ms/step - loss: 0.3102 - acc: 0.8901 - val_loss: 0.8303 - val_acc: 0.7933\n",
      "Epoch 253/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3066 - acc: 0.8896 - val_loss: 0.9091 - val_acc: 0.7883\n",
      "Epoch 254/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3069 - acc: 0.8869 - val_loss: 1.0711 - val_acc: 0.7467\n",
      "Epoch 255/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3027 - acc: 0.8960 - val_loss: 1.0987 - val_acc: 0.7433\n",
      "Epoch 256/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3141 - acc: 0.8907 - val_loss: 1.1623 - val_acc: 0.7367\n",
      "Epoch 257/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2986 - acc: 0.8931 - val_loss: 0.8037 - val_acc: 0.7967\n",
      "Epoch 258/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2967 - acc: 0.8969 - val_loss: 0.8782 - val_acc: 0.7689\n",
      "Epoch 259/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3173 - acc: 0.8913 - val_loss: 0.8855 - val_acc: 0.7894\n",
      "Epoch 260/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2943 - acc: 0.8962 - val_loss: 1.2430 - val_acc: 0.7228\n",
      "Epoch 261/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3005 - acc: 0.8986 - val_loss: 1.0209 - val_acc: 0.7494\n",
      "Epoch 262/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3026 - acc: 0.8915 - val_loss: 0.9112 - val_acc: 0.7800\n",
      "Epoch 263/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2945 - acc: 0.8944 - val_loss: 1.1437 - val_acc: 0.7483\n",
      "Epoch 264/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.3080 - acc: 0.8939 - val_loss: 1.0801 - val_acc: 0.7533\n",
      "Epoch 265/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2981 - acc: 0.8950 - val_loss: 0.9367 - val_acc: 0.7700\n",
      "Epoch 266/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2852 - acc: 0.9032 - val_loss: 1.2152 - val_acc: 0.7339\n",
      "Epoch 267/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.3065 - acc: 0.8950 - val_loss: 0.7827 - val_acc: 0.8039\n",
      "Epoch 268/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.2801 - acc: 0.9039 - val_loss: 1.0003 - val_acc: 0.7711\n",
      "Epoch 269/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2983 - acc: 0.8947 - val_loss: 1.0000 - val_acc: 0.7867\n",
      "Epoch 270/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2861 - acc: 0.9031 - val_loss: 1.0587 - val_acc: 0.7578\n",
      "Epoch 271/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2920 - acc: 0.9008 - val_loss: 0.9577 - val_acc: 0.7906\n",
      "Epoch 272/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2769 - acc: 0.9028 - val_loss: 0.8191 - val_acc: 0.8011\n",
      "Epoch 273/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.2957 - acc: 0.8961 - val_loss: 1.3218 - val_acc: 0.7244\n",
      "Epoch 274/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2831 - acc: 0.9007 - val_loss: 0.8833 - val_acc: 0.7861\n",
      "Epoch 275/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2755 - acc: 0.9054 - val_loss: 0.9079 - val_acc: 0.8006\n",
      "Epoch 276/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2855 - acc: 0.8986 - val_loss: 0.9810 - val_acc: 0.7761\n",
      "Epoch 277/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.2725 - acc: 0.9012 - val_loss: 0.9272 - val_acc: 0.7844\n",
      "Epoch 278/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2745 - acc: 0.9019 - val_loss: 0.9134 - val_acc: 0.7850\n",
      "Epoch 279/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2825 - acc: 0.9058 - val_loss: 1.0968 - val_acc: 0.7411\n",
      "Epoch 280/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2848 - acc: 0.9049 - val_loss: 1.1875 - val_acc: 0.7356\n",
      "Epoch 281/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2752 - acc: 0.9078 - val_loss: 1.3270 - val_acc: 0.7122\n",
      "Epoch 282/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2738 - acc: 0.9033 - val_loss: 0.8400 - val_acc: 0.7972\n",
      "Epoch 283/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2838 - acc: 0.9038 - val_loss: 1.5264 - val_acc: 0.6578\n",
      "Epoch 284/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2878 - acc: 0.9014 - val_loss: 1.0345 - val_acc: 0.7494\n",
      "Epoch 285/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2771 - acc: 0.9036 - val_loss: 1.0856 - val_acc: 0.7550\n",
      "Epoch 286/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2589 - acc: 0.9090 - val_loss: 1.2236 - val_acc: 0.7250\n",
      "Epoch 287/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2727 - acc: 0.9096 - val_loss: 1.0262 - val_acc: 0.7694\n",
      "Epoch 288/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2686 - acc: 0.8990 - val_loss: 0.9872 - val_acc: 0.7789\n",
      "Epoch 289/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.2688 - acc: 0.9072 - val_loss: 1.0102 - val_acc: 0.7578\n",
      "Epoch 290/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2658 - acc: 0.9057 - val_loss: 1.0781 - val_acc: 0.7550\n",
      "Epoch 291/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2506 - acc: 0.9129 - val_loss: 0.9335 - val_acc: 0.7711\n",
      "Epoch 292/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2554 - acc: 0.9117 - val_loss: 1.2881 - val_acc: 0.7172\n",
      "Epoch 293/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.2535 - acc: 0.9129 - val_loss: 0.9049 - val_acc: 0.7878\n",
      "Epoch 294/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.2608 - acc: 0.9075 - val_loss: 1.0311 - val_acc: 0.7567\n",
      "Epoch 295/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2800 - acc: 0.9017 - val_loss: 1.3587 - val_acc: 0.7200\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2645 - acc: 0.9082 - val_loss: 1.4556 - val_acc: 0.6994\n",
      "Epoch 297/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2656 - acc: 0.9056 - val_loss: 1.1653 - val_acc: 0.7483\n",
      "Epoch 298/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2571 - acc: 0.9117 - val_loss: 0.9150 - val_acc: 0.7872\n",
      "Epoch 299/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2588 - acc: 0.9067 - val_loss: 0.9287 - val_acc: 0.7872\n",
      "Epoch 300/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2706 - acc: 0.9031 - val_loss: 0.8955 - val_acc: 0.7861\n",
      "Epoch 301/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2697 - acc: 0.9064 - val_loss: 1.4051 - val_acc: 0.7317\n",
      "Epoch 302/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.2659 - acc: 0.9035 - val_loss: 1.2167 - val_acc: 0.7494\n",
      "Epoch 303/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2597 - acc: 0.9113 - val_loss: 1.1803 - val_acc: 0.7506\n",
      "Epoch 304/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2508 - acc: 0.9132 - val_loss: 0.9724 - val_acc: 0.7739\n",
      "Epoch 305/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2470 - acc: 0.9125 - val_loss: 1.0179 - val_acc: 0.7422\n",
      "Epoch 306/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2404 - acc: 0.9168 - val_loss: 1.2186 - val_acc: 0.7444\n",
      "Epoch 307/1000\n",
      "225/225 [==============================] - 48s 213ms/step - loss: 0.2524 - acc: 0.9132 - val_loss: 0.9068 - val_acc: 0.7833\n",
      "Epoch 308/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2675 - acc: 0.9031 - val_loss: 0.9427 - val_acc: 0.7722\n",
      "Epoch 309/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2476 - acc: 0.9140 - val_loss: 1.1534 - val_acc: 0.7244\n",
      "Epoch 310/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2433 - acc: 0.9117 - val_loss: 0.9084 - val_acc: 0.7994\n",
      "Epoch 311/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2350 - acc: 0.9190 - val_loss: 1.0304 - val_acc: 0.7650\n",
      "Epoch 312/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2431 - acc: 0.9158 - val_loss: 1.0019 - val_acc: 0.7806\n",
      "Epoch 313/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2287 - acc: 0.9212 - val_loss: 1.0279 - val_acc: 0.7739\n",
      "Epoch 314/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2444 - acc: 0.9137 - val_loss: 0.9868 - val_acc: 0.7756\n",
      "Epoch 315/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.2470 - acc: 0.9154 - val_loss: 1.2095 - val_acc: 0.7433\n",
      "Epoch 316/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2315 - acc: 0.9153 - val_loss: 1.1808 - val_acc: 0.7500\n",
      "Epoch 317/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2464 - acc: 0.9149 - val_loss: 0.9980 - val_acc: 0.7711\n",
      "Epoch 318/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2440 - acc: 0.9113 - val_loss: 0.9884 - val_acc: 0.7583\n",
      "Epoch 319/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2485 - acc: 0.9139 - val_loss: 1.1160 - val_acc: 0.7561\n",
      "Epoch 320/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.2350 - acc: 0.9169 - val_loss: 1.0375 - val_acc: 0.7722\n",
      "Epoch 321/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2370 - acc: 0.9196 - val_loss: 1.2664 - val_acc: 0.7472\n",
      "Epoch 322/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2426 - acc: 0.9140 - val_loss: 1.1222 - val_acc: 0.7600\n",
      "Epoch 323/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2425 - acc: 0.9165 - val_loss: 0.8206 - val_acc: 0.8072\n",
      "Epoch 324/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2423 - acc: 0.9150 - val_loss: 1.0376 - val_acc: 0.7639\n",
      "Epoch 325/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2252 - acc: 0.9215 - val_loss: 1.3769 - val_acc: 0.7206\n",
      "Epoch 326/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2270 - acc: 0.9176 - val_loss: 0.9576 - val_acc: 0.7861\n",
      "Epoch 327/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2293 - acc: 0.9200 - val_loss: 0.9804 - val_acc: 0.7817\n",
      "Epoch 328/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2354 - acc: 0.9193 - val_loss: 1.0510 - val_acc: 0.7750\n",
      "Epoch 329/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2267 - acc: 0.9214 - val_loss: 1.1617 - val_acc: 0.7567\n",
      "Epoch 330/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2384 - acc: 0.9165 - val_loss: 0.9010 - val_acc: 0.7861\n",
      "Epoch 331/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2335 - acc: 0.9194 - val_loss: 0.9589 - val_acc: 0.7844\n",
      "Epoch 332/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2252 - acc: 0.9212 - val_loss: 0.9021 - val_acc: 0.7900\n",
      "Epoch 333/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.2226 - acc: 0.9199 - val_loss: 0.8906 - val_acc: 0.8011\n",
      "Epoch 334/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2202 - acc: 0.9225 - val_loss: 1.0332 - val_acc: 0.7722\n",
      "Epoch 335/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.2384 - acc: 0.9144 - val_loss: 1.1379 - val_acc: 0.7644\n",
      "Epoch 336/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2103 - acc: 0.9269 - val_loss: 1.0425 - val_acc: 0.7689\n",
      "Epoch 337/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2211 - acc: 0.9221 - val_loss: 1.1752 - val_acc: 0.7661\n",
      "Epoch 338/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.2278 - acc: 0.9206 - val_loss: 1.0105 - val_acc: 0.7828\n",
      "Epoch 339/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2404 - acc: 0.9194 - val_loss: 1.0593 - val_acc: 0.7661\n",
      "Epoch 340/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2268 - acc: 0.9178 - val_loss: 1.0706 - val_acc: 0.7706\n",
      "Epoch 341/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.2270 - acc: 0.9212 - val_loss: 0.8987 - val_acc: 0.8022\n",
      "Epoch 342/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2260 - acc: 0.9182 - val_loss: 1.0990 - val_acc: 0.7633\n",
      "Epoch 343/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.2211 - acc: 0.9242 - val_loss: 0.8603 - val_acc: 0.7994\n",
      "Epoch 344/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2239 - acc: 0.9207 - val_loss: 0.7935 - val_acc: 0.8128\n",
      "Epoch 345/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.2059 - acc: 0.9292 - val_loss: 1.0595 - val_acc: 0.7706\n",
      "Epoch 346/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2152 - acc: 0.9256 - val_loss: 0.9779 - val_acc: 0.7939\n",
      "Epoch 347/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2226 - acc: 0.9261 - val_loss: 0.8355 - val_acc: 0.8117\n",
      "Epoch 348/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2226 - acc: 0.9208 - val_loss: 0.7852 - val_acc: 0.8128\n",
      "Epoch 349/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2142 - acc: 0.9247 - val_loss: 0.8109 - val_acc: 0.8194\n",
      "Epoch 350/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2067 - acc: 0.9299 - val_loss: 0.8228 - val_acc: 0.8117\n",
      "Epoch 351/1000\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2067 - acc: 0.9286 - val_loss: 1.1575 - val_acc: 0.7383\n",
      "Epoch 352/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.2219 - acc: 0.9232 - val_loss: 0.9615 - val_acc: 0.7817\n",
      "Epoch 353/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.2023 - acc: 0.9296 - val_loss: 1.1776 - val_acc: 0.7583\n",
      "Epoch 354/1000\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.2054 - acc: 0.9300 - val_loss: 1.0704 - val_acc: 0.7778\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36/225 [===>..........................] - ETA: 28s - loss: 0.2023 - acc: 0.9314"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b5b7659cbd73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m model.fit_generator(generator, steps_per_epoch=len(X_train)/32, epochs=1000, validation_data=(X_train_validation, y_train_validation), \n\u001b[1;32m---> 26\u001b[1;33m          callbacks = [mcp])\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m                 \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    683\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "mcp = ModelCheckpoint(\"models/best_model_3splits_mel.h5\", monitor='val_acc', verbose=0, \n",
    "                      save_best_only=True, save_weights_only=False, mode='max', period=1)\n",
    "\n",
    "# do crazy image generator stuff to randomize data\n",
    "gen = ImageDataGenerator(horizontal_flip = True,\n",
    "                         vertical_flip = True,\n",
    "                         width_shift_range = 0.1,\n",
    "                         height_shift_range = 0.1,\n",
    "                         zoom_range = 0.1,\n",
    "                         rotation_range = 10\n",
    "                        )\n",
    "generator = gen.flow(X_train, y_train, batch_size = 32)\n",
    "# end of crazy image genertor stuff\n",
    "\n",
    "\n",
    "# adam = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0, amsgrad=False)\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model = build_model()\n",
    "model.compile(sgd, 'categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(generator, steps_per_epoch=len(X_train)/32, epochs=1000, validation_data=(X_train_validation, y_train_validation), \n",
    "         callbacks = [mcp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete the below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "225/225 [==============================] - 54s 238ms/step - loss: 0.5521 - acc: 0.8147 - val_loss: 0.7215 - val_acc: 0.7583\n",
      "Epoch 2/100\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.5568 - acc: 0.8189 - val_loss: 0.6804 - val_acc: 0.7694\n",
      "Epoch 3/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.5533 - acc: 0.8149 - val_loss: 0.7528 - val_acc: 0.7506\n",
      "Epoch 4/100\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.5434 - acc: 0.8194 - val_loss: 0.6099 - val_acc: 0.8011\n",
      "Epoch 5/100\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.5235 - acc: 0.8233 - val_loss: 0.6872 - val_acc: 0.7661\n",
      "Epoch 6/100\n",
      "225/225 [==============================] - 49s 218ms/step - loss: 0.5426 - acc: 0.8208 - val_loss: 0.6453 - val_acc: 0.7878\n",
      "Epoch 7/100\n",
      "225/225 [==============================] - 50s 222ms/step - loss: 0.5349 - acc: 0.8237 - val_loss: 0.6041 - val_acc: 0.8067\n",
      "Epoch 8/100\n",
      "225/225 [==============================] - 50s 220ms/step - loss: 0.5118 - acc: 0.8268 - val_loss: 0.6638 - val_acc: 0.7767\n",
      "Epoch 9/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.5410 - acc: 0.8231 - val_loss: 0.6863 - val_acc: 0.7667\n",
      "Epoch 10/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.5108 - acc: 0.8332 - val_loss: 0.5817 - val_acc: 0.8083\n",
      "Epoch 11/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.5315 - acc: 0.8257 - val_loss: 0.6639 - val_acc: 0.7856\n",
      "Epoch 12/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.5059 - acc: 0.8308 - val_loss: 0.6207 - val_acc: 0.8061\n",
      "Epoch 13/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.5178 - acc: 0.8271 - val_loss: 0.6520 - val_acc: 0.7850\n",
      "Epoch 14/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.4819 - acc: 0.8400 - val_loss: 0.9173 - val_acc: 0.7311\n",
      "Epoch 15/100\n",
      "225/225 [==============================] - 49s 218ms/step - loss: 0.4934 - acc: 0.8379 - val_loss: 0.6728 - val_acc: 0.7900\n",
      "Epoch 16/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.4879 - acc: 0.8410 - val_loss: 0.6695 - val_acc: 0.7811\n",
      "Epoch 17/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.4779 - acc: 0.8417 - val_loss: 0.7366 - val_acc: 0.7578\n",
      "Epoch 18/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.4957 - acc: 0.8374 - val_loss: 0.7375 - val_acc: 0.7539\n",
      "Epoch 19/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.4761 - acc: 0.8443 - val_loss: 0.6793 - val_acc: 0.7878\n",
      "Epoch 20/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.4968 - acc: 0.8337 - val_loss: 0.6469 - val_acc: 0.7856\n",
      "Epoch 21/100\n",
      "225/225 [==============================] - 49s 218ms/step - loss: 0.4743 - acc: 0.8357 - val_loss: 0.9549 - val_acc: 0.7261\n",
      "Epoch 22/100\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.4873 - acc: 0.8386 - val_loss: 0.7086 - val_acc: 0.7717\n",
      "Epoch 23/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.4716 - acc: 0.8464 - val_loss: 0.6999 - val_acc: 0.7778\n",
      "Epoch 24/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.4608 - acc: 0.8458 - val_loss: 0.6029 - val_acc: 0.7950\n",
      "Epoch 25/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.4552 - acc: 0.8497 - val_loss: 0.8339 - val_acc: 0.7467\n",
      "Epoch 26/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.4554 - acc: 0.8450 - val_loss: 0.6060 - val_acc: 0.7972\n",
      "Epoch 27/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.4488 - acc: 0.8518 - val_loss: 0.7525 - val_acc: 0.7667\n",
      "Epoch 28/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.4241 - acc: 0.8600 - val_loss: 0.6795 - val_acc: 0.7756\n",
      "Epoch 29/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.4377 - acc: 0.8546 - val_loss: 0.7591 - val_acc: 0.7600\n",
      "Epoch 30/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.4186 - acc: 0.8617 - val_loss: 0.7125 - val_acc: 0.7778\n",
      "Epoch 31/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.4296 - acc: 0.8561 - val_loss: 0.7200 - val_acc: 0.7789\n",
      "Epoch 32/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.4291 - acc: 0.8567 - val_loss: 0.6136 - val_acc: 0.8144\n",
      "Epoch 33/100\n",
      "225/225 [==============================] - 49s 218ms/step - loss: 0.4116 - acc: 0.8607 - val_loss: 0.8203 - val_acc: 0.7544\n",
      "Epoch 34/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.4342 - acc: 0.8567 - val_loss: 0.8540 - val_acc: 0.7578\n",
      "Epoch 35/100\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.4430 - acc: 0.8553 - val_loss: 0.8005 - val_acc: 0.7744\n",
      "Epoch 36/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.3980 - acc: 0.8657 - val_loss: 0.6420 - val_acc: 0.7872\n",
      "Epoch 37/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.4126 - acc: 0.8649 - val_loss: 0.7519 - val_acc: 0.7739\n",
      "Epoch 38/100\n",
      "225/225 [==============================] - 49s 219ms/step - loss: 0.4072 - acc: 0.8643 - val_loss: 0.8941 - val_acc: 0.7472\n",
      "Epoch 39/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.3938 - acc: 0.8689 - val_loss: 0.6376 - val_acc: 0.8150\n",
      "Epoch 40/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.3899 - acc: 0.8660 - val_loss: 0.7962 - val_acc: 0.7783\n",
      "Epoch 41/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.4089 - acc: 0.8643 - val_loss: 0.7564 - val_acc: 0.7733\n",
      "Epoch 42/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.3974 - acc: 0.8714 - val_loss: 0.5916 - val_acc: 0.8117\n",
      "Epoch 43/100\n",
      "225/225 [==============================] - 49s 219ms/step - loss: 0.3786 - acc: 0.8732 - val_loss: 0.6347 - val_acc: 0.8061\n",
      "Epoch 44/100\n",
      "225/225 [==============================] - 50s 220ms/step - loss: 0.3722 - acc: 0.8782 - val_loss: 0.6312 - val_acc: 0.8094\n",
      "Epoch 45/100\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.3788 - acc: 0.8706 - val_loss: 0.6120 - val_acc: 0.8239\n",
      "Epoch 46/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.3737 - acc: 0.8733 - val_loss: 0.7424 - val_acc: 0.7850\n",
      "Epoch 47/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.3616 - acc: 0.8831 - val_loss: 0.6222 - val_acc: 0.8061\n",
      "Epoch 48/100\n",
      "225/225 [==============================] - 49s 218ms/step - loss: 0.3703 - acc: 0.8775 - val_loss: 0.6094 - val_acc: 0.8200\n",
      "Epoch 49/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.3631 - acc: 0.8825 - val_loss: 0.6302 - val_acc: 0.8178\n",
      "Epoch 50/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.3619 - acc: 0.8812 - val_loss: 0.8198 - val_acc: 0.7711\n",
      "Epoch 51/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.3762 - acc: 0.8746 - val_loss: 0.9783 - val_acc: 0.7406\n",
      "Epoch 52/100\n",
      "225/225 [==============================] - 49s 218ms/step - loss: 0.3739 - acc: 0.8733 - val_loss: 0.8779 - val_acc: 0.7667\n",
      "Epoch 53/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.3735 - acc: 0.8789 - val_loss: 0.7205 - val_acc: 0.7956\n",
      "Epoch 54/100\n",
      "225/225 [==============================] - 49s 219ms/step - loss: 0.3517 - acc: 0.8810 - val_loss: 0.6148 - val_acc: 0.8139\n",
      "Epoch 55/100\n",
      "225/225 [==============================] - 50s 222ms/step - loss: 0.3478 - acc: 0.8815 - val_loss: 0.6318 - val_acc: 0.8128\n",
      "Epoch 56/100\n",
      "225/225 [==============================] - 49s 219ms/step - loss: 0.3755 - acc: 0.8760 - val_loss: 0.7801 - val_acc: 0.7817\n",
      "Epoch 57/100\n",
      "225/225 [==============================] - 49s 219ms/step - loss: 0.3431 - acc: 0.8857 - val_loss: 0.7908 - val_acc: 0.7667\n",
      "Epoch 58/100\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.3505 - acc: 0.8824 - val_loss: 0.6767 - val_acc: 0.8100\n",
      "Epoch 59/100\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.3359 - acc: 0.8887 - val_loss: 0.6521 - val_acc: 0.8206\n",
      "Epoch 60/100\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.3377 - acc: 0.8832 - val_loss: 0.8754 - val_acc: 0.7644\n",
      "Epoch 61/100\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.3430 - acc: 0.8815 - val_loss: 0.5626 - val_acc: 0.8350\n",
      "Epoch 62/100\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.3322 - acc: 0.8887 - val_loss: 0.7725 - val_acc: 0.7833\n",
      "Epoch 63/100\n",
      "225/225 [==============================] - 49s 219ms/step - loss: 0.3558 - acc: 0.8815 - val_loss: 0.6879 - val_acc: 0.8011\n",
      "Epoch 64/100\n",
      "225/225 [==============================] - 51s 228ms/step - loss: 0.3323 - acc: 0.8881 - val_loss: 0.6929 - val_acc: 0.8178\n",
      "Epoch 65/100\n",
      "225/225 [==============================] - 50s 221ms/step - loss: 0.3293 - acc: 0.8919 - val_loss: 0.6388 - val_acc: 0.8111\n",
      "Epoch 66/100\n",
      "225/225 [==============================] - 49s 218ms/step - loss: 0.3243 - acc: 0.8939 - val_loss: 0.6456 - val_acc: 0.8150\n",
      "Epoch 67/100\n",
      "225/225 [==============================] - 49s 218ms/step - loss: 0.3084 - acc: 0.8971 - val_loss: 0.7431 - val_acc: 0.7972\n",
      "Epoch 68/100\n",
      "225/225 [==============================] - 49s 219ms/step - loss: 0.3165 - acc: 0.8939 - val_loss: 0.6644 - val_acc: 0.8039\n",
      "Epoch 69/100\n",
      "225/225 [==============================] - 49s 218ms/step - loss: 0.3228 - acc: 0.8942 - val_loss: 0.6792 - val_acc: 0.8039\n",
      "Epoch 70/100\n",
      "225/225 [==============================] - 49s 219ms/step - loss: 0.3353 - acc: 0.8839 - val_loss: 0.6613 - val_acc: 0.8178\n",
      "Epoch 71/100\n",
      "225/225 [==============================] - 49s 219ms/step - loss: 0.3124 - acc: 0.8929 - val_loss: 0.6627 - val_acc: 0.8144\n",
      "Epoch 72/100\n",
      "225/225 [==============================] - 49s 219ms/step - loss: 0.3046 - acc: 0.9000 - val_loss: 0.6059 - val_acc: 0.8167\n",
      "Epoch 73/100\n",
      "225/225 [==============================] - 49s 219ms/step - loss: 0.2891 - acc: 0.9039 - val_loss: 0.7664 - val_acc: 0.7883\n",
      "Epoch 74/100\n",
      "225/225 [==============================] - 50s 221ms/step - loss: 0.3071 - acc: 0.8969 - val_loss: 0.9875 - val_acc: 0.7450\n",
      "Epoch 75/100\n",
      "225/225 [==============================] - 49s 219ms/step - loss: 0.3046 - acc: 0.8979 - val_loss: 0.7759 - val_acc: 0.7922\n",
      "Epoch 76/100\n",
      "225/225 [==============================] - 49s 218ms/step - loss: 0.2931 - acc: 0.9019 - val_loss: 0.7620 - val_acc: 0.7889\n",
      "Epoch 77/100\n",
      "225/225 [==============================] - 49s 220ms/step - loss: 0.2967 - acc: 0.9021 - val_loss: 0.8275 - val_acc: 0.7700\n",
      "Epoch 78/100\n",
      "225/225 [==============================] - 49s 218ms/step - loss: 0.2983 - acc: 0.9018 - val_loss: 0.7327 - val_acc: 0.8044\n",
      "Epoch 79/100\n",
      "225/225 [==============================] - 49s 219ms/step - loss: 0.2756 - acc: 0.9075 - val_loss: 0.8162 - val_acc: 0.7750\n",
      "Epoch 80/100\n",
      "225/225 [==============================] - 50s 221ms/step - loss: 0.2828 - acc: 0.9053 - val_loss: 0.8326 - val_acc: 0.7650\n",
      "Epoch 81/100\n",
      "225/225 [==============================] - 50s 223ms/step - loss: 0.2881 - acc: 0.9038 - val_loss: 0.7505 - val_acc: 0.7939\n",
      "Epoch 82/100\n",
      "225/225 [==============================] - 66s 292ms/step - loss: 0.2919 - acc: 0.9047 - val_loss: 0.7306 - val_acc: 0.8017\n",
      "Epoch 83/100\n",
      "225/225 [==============================] - 64s 284ms/step - loss: 0.3001 - acc: 0.8972 - val_loss: 0.6346 - val_acc: 0.8228\n",
      "Epoch 84/100\n",
      "225/225 [==============================] - 51s 227ms/step - loss: 0.3018 - acc: 0.8990 - val_loss: 0.8665 - val_acc: 0.7844\n",
      "Epoch 85/100\n",
      "225/225 [==============================] - 50s 224ms/step - loss: 0.2783 - acc: 0.9086 - val_loss: 0.6235 - val_acc: 0.8189\n",
      "Epoch 86/100\n",
      "225/225 [==============================] - 51s 225ms/step - loss: 0.2677 - acc: 0.9103 - val_loss: 0.7254 - val_acc: 0.8056\n",
      "Epoch 87/100\n",
      "225/225 [==============================] - 50s 223ms/step - loss: 0.2806 - acc: 0.9069 - val_loss: 0.6780 - val_acc: 0.8144\n",
      "Epoch 88/100\n",
      " 32/225 [===>..........................] - ETA: 28s - loss: 0.2700 - acc: 0.9121"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d5b63486cc26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m model.fit_generator(generator, steps_per_epoch=len(X_train)/32, epochs=100, validation_data=(X_train_validation, y_train_validation), \n\u001b[1;32m---> 26\u001b[1;33m          callbacks = [mcp])\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "mcp = ModelCheckpoint(\"models/best_model_3splits_mel.h5\", monitor='val_acc', verbose=0, \n",
    "                      save_best_only=True, save_weights_only=False, mode='max', period=1)\n",
    "\n",
    "# do crazy image generator stuff to randomize data\n",
    "gen = ImageDataGenerator(horizontal_flip = True,\n",
    "                         vertical_flip = True,\n",
    "                         width_shift_range = 0.1,\n",
    "                         height_shift_range = 0.1,\n",
    "                         zoom_range = 0.1,\n",
    "                         rotation_range = 10\n",
    "                        )\n",
    "generator = gen.flow(X_train, y_train, batch_size = 32)\n",
    "# end of crazy image genertor stuff\n",
    "\n",
    "\n",
    "# adam = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0, amsgrad=False)\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model = load_model(\"models/best_model_3splits_mel.h5\")\n",
    "model.compile(sgd, 'categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(generator, steps_per_epoch=len(X_train)/32, epochs=100, validation_data=(X_train_validation, y_train_validation), \n",
    "         callbacks = [mcp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/trained_music_classifier.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 1ms/step\n",
      "[3.419465198516846, 0.5800000071525574]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_train_validation, y_train_validation, batch_size=32, verbose=1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build second classifier, ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8996, 130)\n",
      "(8996, 10)\n",
      "Validation x train set:(1800, 130)\n",
      "X train set:(7196, 130)\n",
      "Validation y train set:(1800, 10)\n",
      "Y train set:(7196, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train_speccen = np.load(\"X_spec_cen_train_3.dat\")\n",
    "y_train_speccen = np.load(\"y_spec_cen_train_3.dat\")\n",
    "\n",
    "X_train_speccen, y_train_speccen = shuffle(X_train_speccen, y_train_speccen, random_state=16)\n",
    "\n",
    "print(X_train_speccen.shape)\n",
    "print(y_train_speccen.shape)\n",
    "\n",
    "\n",
    "# split data into validation set\n",
    "training_set_size = int(X_train_speccen.shape[0] * .80)\n",
    "X_train_speccen_validation = X_train_speccen[training_set_size:, :]\n",
    "y_train_speccen_validation = y_train_speccen[training_set_size:, :]\n",
    "y_train_speccen = y_train_speccen[:training_set_size,:]\n",
    "X_train_speccen = X_train_speccen[:training_set_size, :]\n",
    "print(\"Validation x train set:\" + str(X_train_speccen_validation.shape))\n",
    "print(\"X train set:\" + str(X_train_speccen.shape))\n",
    "print(\"Validation y train set:\" + str(y_train_speccen_validation.shape))\n",
    "print(\"Y train set:\" + str(y_train_speccen.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7196 samples, validate on 1800 samples\n",
      "Epoch 1/50\n",
      "7196/7196 [==============================] - 6s 768us/step - loss: 2.0209 - acc: 0.2910 - val_loss: 1.9207 - val_acc: 0.3233\n",
      "Epoch 2/50\n",
      "7196/7196 [==============================] - 3s 353us/step - loss: 1.7905 - acc: 0.3495 - val_loss: 1.8139 - val_acc: 0.3383\n",
      "Epoch 3/50\n",
      "7196/7196 [==============================] - 3s 354us/step - loss: 1.7377 - acc: 0.3717 - val_loss: 1.7845 - val_acc: 0.3656\n",
      "Epoch 4/50\n",
      "7196/7196 [==============================] - 3s 354us/step - loss: 1.6802 - acc: 0.3936 - val_loss: 1.7809 - val_acc: 0.3600\n",
      "Epoch 5/50\n",
      "7196/7196 [==============================] - 3s 351us/step - loss: 1.6288 - acc: 0.4152 - val_loss: 1.8188 - val_acc: 0.3178\n",
      "Epoch 6/50\n",
      "7196/7196 [==============================] - 3s 361us/step - loss: 1.6013 - acc: 0.4234 - val_loss: 1.7971 - val_acc: 0.3578\n",
      "Epoch 7/50\n",
      "7196/7196 [==============================] - 3s 364us/step - loss: 1.5675 - acc: 0.4362 - val_loss: 1.8158 - val_acc: 0.3428\n",
      "Epoch 8/50\n",
      "7196/7196 [==============================] - 3s 356us/step - loss: 1.5094 - acc: 0.4532 - val_loss: 1.8246 - val_acc: 0.3506\n",
      "Epoch 9/50\n",
      "7196/7196 [==============================] - 3s 357us/step - loss: 1.4797 - acc: 0.4704 - val_loss: 1.8245 - val_acc: 0.3600\n",
      "Epoch 10/50\n",
      "7196/7196 [==============================] - 3s 356us/step - loss: 1.4526 - acc: 0.4808 - val_loss: 1.8423 - val_acc: 0.3606\n",
      "Epoch 11/50\n",
      "7196/7196 [==============================] - 3s 366us/step - loss: 1.4115 - acc: 0.4960 - val_loss: 1.8330 - val_acc: 0.3567\n",
      "Epoch 12/50\n",
      "7196/7196 [==============================] - 3s 361us/step - loss: 1.3648 - acc: 0.5124 - val_loss: 1.9063 - val_acc: 0.3600\n",
      "Epoch 13/50\n",
      "7196/7196 [==============================] - 3s 355us/step - loss: 1.3447 - acc: 0.5238 - val_loss: 1.8837 - val_acc: 0.3772\n",
      "Epoch 14/50\n",
      "7196/7196 [==============================] - 3s 360us/step - loss: 1.2995 - acc: 0.5424 - val_loss: 1.9358 - val_acc: 0.3606\n",
      "Epoch 15/50\n",
      "7196/7196 [==============================] - 3s 371us/step - loss: 1.2758 - acc: 0.5571 - val_loss: 1.9313 - val_acc: 0.3622\n",
      "Epoch 16/50\n",
      "7196/7196 [==============================] - 3s 360us/step - loss: 1.2393 - acc: 0.5675 - val_loss: 2.0124 - val_acc: 0.3461\n",
      "Epoch 17/50\n",
      "7196/7196 [==============================] - 3s 359us/step - loss: 1.2216 - acc: 0.5703 - val_loss: 1.9814 - val_acc: 0.3767\n",
      "Epoch 18/50\n",
      "7196/7196 [==============================] - 3s 359us/step - loss: 1.1912 - acc: 0.5845 - val_loss: 1.9725 - val_acc: 0.3717\n",
      "Epoch 19/50\n",
      "7196/7196 [==============================] - 3s 363us/step - loss: 1.1388 - acc: 0.6002 - val_loss: 2.1268 - val_acc: 0.3328\n",
      "Epoch 20/50\n",
      "7196/7196 [==============================] - 3s 363us/step - loss: 1.1370 - acc: 0.6014 - val_loss: 2.0481 - val_acc: 0.3611\n",
      "Epoch 21/50\n",
      "7196/7196 [==============================] - 3s 360us/step - loss: 1.0995 - acc: 0.6160 - val_loss: 2.0705 - val_acc: 0.3706\n",
      "Epoch 22/50\n",
      "3680/7196 [==============>...............] - ETA: 1s - loss: 1.0782 - acc: 0.6253"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-8e6e3b4da1c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m ann_model.fit(X_train_speccen, y_train_speccen, batch_size=32, epochs=50 , \n\u001b[1;32m---> 45\u001b[1;33m               validation_data=(X_train_speccen_validation, y_train_speccen_validation), callbacks = [mcp])\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ann_model = Sequential()\n",
    "\n",
    "# ann_model.add(Flatten())\n",
    "ann_model.add(BatchNormalization())\n",
    "ann_model.add(Dense(128, activation = 'relu', input_shape = X_train.shape[1:]))\n",
    "#ann_model.add(Dropout(0.5))\n",
    "\n",
    "ann_model.add(BatchNormalization())\n",
    "ann_model.add(Dense(256, activation = 'relu'))\n",
    "#ann_model.add(Dropout(0.5))\n",
    "\n",
    "# ann_model.add(BatchNormalization())\n",
    "# ann_model.add(Dense(512, activation = 'relu'))\n",
    "# #ann_model.add(Dropout(0.5))\n",
    "\n",
    "# ann_model.add(BatchNormalization())\n",
    "# ann_model.add(Dense(1024, activation = 'relu'))\n",
    "# #ann_model.add(Dropout(0.5))\n",
    "\n",
    "# ann_model.add(BatchNormalization())\n",
    "# ann_model.add(Dense(512, activation = 'relu'))\n",
    "\n",
    "# ann_model.add(BatchNormalization())\n",
    "# ann_model.add(Dense(256, activation = 'relu'))\n",
    "\n",
    "ann_model.add(BatchNormalization())\n",
    "ann_model.add(Dense(64, activation = 'relu'))\n",
    "\n",
    "ann_model.add(BatchNormalization())\n",
    "ann_model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "mcp = ModelCheckpoint(\"models/speccen_ann.h5\", monitor='val_acc', verbose=0, \n",
    "                      save_best_only=True, save_weights_only=False, mode='max', period=1)\n",
    "\n",
    "#adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.001, amsgrad=False)\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# Using custom adam is horrifically bad for ANN...\n",
    "ann_model.compile('adam', 'categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "ann_model.fit(X_train_speccen, y_train_speccen, batch_size=32, epochs=50 , \n",
    "              validation_data=(X_train_speccen_validation, y_train_speccen_validation), callbacks = [mcp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 [==============================] - 0s 60us/step\n",
      "[15.969083701239692, 0.0]\n"
     ]
    }
   ],
   "source": [
    "score = ann_model.evaluate(X_train_validation, y_train_validation, batch_size=32, verbose=1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "#### Default adam optimizer, 1 conv layer 128, and a FC layer of 128:\n",
    "    - No convergence\n",
    "#### Default adam optimizer, 2 conv layer 8 filters, and a FC layer of 16:\n",
    "    - No convergence\n",
    "#### Default adam optimizer, 2 conv layers and a FC layer of 64 neurons:\n",
    "    - No dropout or BN\n",
    "        - 99% train_acc, 60% val_acc, 10 epochs\n",
    "    - Using BatchNormalization\n",
    "        - BN at only FC layer overfit extremely fast and was stuck at about 53% val_acc\n",
    "        - BN at every layer still overfits extremely fast but gets to about 68% val_acc\n",
    "    - No BN, adding Dropout\n",
    "        - 0.75 dropout on just the dense layer before softmax -> 83% train_acc and 57% val_acc, in about 10 epochs\n",
    "        - 0.25 dropout on last layer, get about 97% train_acc with 54% val_acc -> in about 10 epochs\n",
    "        - 0.25 dropout on every layer, 100 epochs: 97% train_Acc with 60% val_acc\n",
    "        - 0.25 dropout on hidden layers, 0.5 dropout on output layer, 100 epochs: 80% train_acc and 58% val_acc\n",
    "        - 0.25 dropout on hidden layers, 0.35 dropout on output layer, 100 epochs: 97% train_acc and 58% val_acc\n",
    "#### Adam optimizer, LR = 0.0001, 2 conv layers and a FC layer of 64 neurons:\n",
    "    - No dropout or BN\n",
    "        - 99% train_acc, 62% val_acc, 20 epochs **not jumpy val_acc** -> starts going down after iteration 29\n",
    "        \n",
    "#### Adam optimizer, LR = 0.00001, 2 conv layers and a FC layer of 64 neurons:\n",
    "    - No dropout or BN\n",
    "        - Still gets up to 99% acc in about 25 epochs\n",
    "        \n",
    "#### SGD, sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True), 5-layer CNN, 64-256, FC 1024\n",
    "    - Dropout only on FC layer 0.5\n",
    "    - BatchNormalization after each activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 118, 168, 64)      9472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 118, 168, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 59, 84, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 53, 78, 128)       401536    \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 53, 78, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 26, 39, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 20, 33, 256)       1605888   \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 20, 33, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 10, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 40960)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                1310752   \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 3,329,898\n",
      "Trainable params: 3,328,938\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "1800/1800 [==============================] - 5s 3ms/step\n",
      "[3 4 7 ... 0 7 7]\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fa59f36eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# build_confusion_matrix()\n",
    "# Builds the confusion matrix for either naive bayes' or logistic regression.\n",
    "# Our goal is to have a strong diagonal which corresponds to good correlation\n",
    "# between validation data classifications and our predictions.\n",
    "# true_classes are the classifications for the predictions and classes are the total range of classes\n",
    "def build_confusion_matrix(predictions, true_classes, classes, file_name):\n",
    "    confusion_matrix = np.zeros((len(classes), len(classes)), dtype='int')\n",
    "    len_pred = len(predictions)\n",
    "    print(true_classes)\n",
    "    # for every class prediction and true class value\n",
    "    for i in range(len_pred):\n",
    "        true_classification = np.where(true_classes[i]==1)\n",
    "#         print(true_classification)\n",
    "        # we hope that these two are equal for a strong diagonal correlation\n",
    "        confusion_matrix[predictions[i], true_classification] += 1\n",
    "\n",
    "    confusion_matrix_df = pd.DataFrame(confusion_matrix, index= classes)\n",
    "    axis_label = [\"blues\", \"classical\", \"country\", \"disco\", \"hiphop\", \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"]\n",
    "    figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.imshow(confusion_matrix_df.values, cmap='jet', interpolation='nearest')\n",
    "    plt.xticks(np.arange(10), axis_label, rotation='85')\n",
    "    plt.yticks(np.arange(10), axis_label)\n",
    "    plt.tick_params(axis='both', labelsize='10')\n",
    "    plt.xlabel(\"True classifications\")\n",
    "    plt.ylabel(\"Predicted classifications\")\n",
    "    plt.title(\"Confusion Matrix of Pred. Classes vs True Classes for Music Classification\")\n",
    "#     plt.tight_layout()\n",
    "    for (j, i), label in np.ndenumerate(confusion_matrix):\n",
    "        if label != 0:\n",
    "            plt.text(i,j,label,ha='center',va='center', size='10')\n",
    "    plt.show()\n",
    "\n",
    "    confusion_matrix_df.to_csv(file_name, sep=\",\", header=classes)\n",
    "\n",
    "model = load_model(\"models/best_model_3splits_mel.h5\")\n",
    "model.summary()\n",
    "\n",
    "predictions = model.predict_classes(X_train_validation, verbose=1)\n",
    "print(predictions)\n",
    "\n",
    "build_confusion_matrix(predictions, y_train_validation, np.arange(0,10), \"confusion_matrix_csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation neural network evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "num_of_folds = 10\n",
    "total_examples = len(X_train)\n",
    "size_of_one_fold = total_examples / 10\n",
    "\n",
    "mcp = ModelCheckpoint(\"models/best_model_3splits_mel.h5\", monitor='val_acc', verbose=0, \n",
    "                      save_best_only=True, save_weights_only=False, mode='max', period=1)\n",
    "\n",
    "# adam = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0, amsgrad=False)\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "\n",
    "# training[:(i-1) * 900] + training[i * 900:]\n",
    "# validation[(i-1) * 900: i*900]\n",
    "\n",
    "for i in range(1, num_of_folds+1):\n",
    "#     print(X_train.shape)\n",
    "    print(\"\\nFold \" + str(i))\n",
    "    \n",
    "    # do cross validation stuff\n",
    "    \n",
    "    X_train_fold = np.concatenate((X_train[:((i-1) * 900), :, :, :], X_train[(i * 900):, :, :, :]))\n",
    "    y_train_fold = np.concatenate((y_train[:((i-1) * 900), :], y_train[(i * 900):, :]))\n",
    "    \n",
    "    X_train_validation_fold = X_train[((i-1) * 900) : (i * 900), :, :, :]\n",
    "    y_train_validation_fold = y_train[((i-1) * 900) : (i * 900), :]\n",
    "    print(\"Validation x train set:\" + str(X_train_validation_fold.shape))\n",
    "    print(\"X train set:\" + str(X_train_fold.shape))\n",
    "    print(\"Validation y train set:\" + str(y_train_validation_fold.shape))\n",
    "    print(\"Y train set:\" + str(y_train_fold.shape) + \"\\n\")\n",
    "    \n",
    "    # end cross-validation\n",
    "    \n",
    "    # get new model to train on\n",
    "    model = build_model()\n",
    "    \n",
    "    model.compile(sgd, 'categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train_fold, y_train_fold, batch_size=32, epochs=3 , validation_data=(X_train_validation_fold, \n",
    "            y_train_validation_fold), callbacks = [mcp])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
